# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xbFSyUpmSLup2-QSEftWGIB7Q37WQSeR
"""
import os
import torch
from transformers import BertTokenizer, BertModel
import numpy as np
from dotenv import load_dotenv
load_dotenv()
mongo_url=os.environ.get('MONGOURL')

# Load the BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')
model = BertModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')

text = "This is an example sentence."
def get_sentence_embedding(text:str):
  # Preprocess the input text
  inputs = tokenizer.encode_plus(
      text,
      add_special_tokens=True,
      max_length=512,
      return_attention_mask=True,
      return_tensors='pt'
  )
  # Get the BERT embeddings
  outputs = model(**inputs)
  last_hidden_states = outputs.last_hidden_state

  # Extract the sentence embedding
  sentence_embedding = torch.mean(last_hidden_states, dim=1)
  return sentence_embedding

#print(get_sentence_embedding(text))

# !pip install torchmetrics
from torchmetrics.functional import pairwise_cosine_similarity

def concat_vector_embeddings(L1:list[str],L2:list[str]):
  x=torch.cat(tuple([get_sentence_embedding(e) for e in L1]),dim=0)
  y=torch.cat(tuple([get_sentence_embedding(e) for e in L2]),dim=0)
  return x,y

def most_similar(L1:list[str],L2:list[str]):
  x,y=concat_vector_embeddings(L1,L2)
  a=pairwise_cosine_similarity(x, y)
  print(a)
  return torch.argmax(a, dim=1)

L1=["J'aime les burgers mcdonalds","voiture"]
L2=["J'aime les burgers mcdonalds","J'aime les voitures","J'aime pas les voitures"]

x,y=concat_vector_embeddings(L1,L2)

print(most_similar(L1,L2))

output = pairwise_cosine_similarity(x, y)
print(output)

import keras

def get_model(n):
    inputs = keras.Input(shape=(768,), name="embeddings")
    x1 = keras.layers.Dense(64, activation="relu")(inputs)
    x2 = keras.layers.Dense(64, activation="relu")(x1)
    outputs = keras.layers.Dense(n, name="predictions")(x2)
    model = keras.Model(inputs=inputs, outputs=outputs)
    return model

#model = get_model(2)
# !pip install pymongo
import pymongo
from keras.utils import Sequence

# Connect to your MongoDB database
client = pymongo.MongoClient(mongo_url)
db = client["RemiGOAT"]
collection = db["Message"]

for x in collection.find():
  criteria = {"_id": x["_id"]}
  embed=get_sentence_embedding(x["body"]).detach().cpu().numpy().tolist()
  update_data = {"$set": {"embedding": embed}}
  result = collection.update_one(criteria, update_data)
  print("ok")


class MongoDataGenerator(Sequence):
    def __init__(self, collection, batch_size):
        self.collection = collection
        self.batch_size = batch_size

    def __len__(self):
        return len(self.collection.find())

    def __getitem__(self, idx):
        batch = []
        for i in range(idx * self.batch_size, (idx + 1) * self.batch_size):
            document = self.collection.find_one(i)
            data = document["data"]
            label = document["label"]
            batch.append((data, label))
        return batch


def train_model(model,epochs,batch_size,train_dataloader):
  optimizer = keras.optimizers.Adam(learning_rate=1e-3)
  loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)
  for epoch in range(epochs):
      print(f"\nStart of epoch {epoch}")
      for step, (inputs, targets) in enumerate(train_dataloader):
          # Forward pass
          logits = model(inputs)
          loss = loss_fn(targets, logits)

          # Backward pass
          model.zero_grad()
          trainable_weights = [v for v in model.trainable_weights]

          # Call torch.Tensor.backward() on the loss to compute gradients
          # for the weights.
          loss.backward()
          gradients = [v.value.grad for v in trainable_weights]

          # Update weights
          with torch.no_grad():
              optimizer.apply(gradients, trainable_weights)

          # Log every 100 batches.
          if step % 100 == 0:
              print(
                  f"Training loss (for 1 batch) at step {step}: {loss.detach().numpy():.4f}"
              )
              print(f"Seen so far: {(step + 1) * batch_size} samples")
  return model

