# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xbFSyUpmSLup2-QSEftWGIB7Q37WQSeR
"""
import os
import sys
import torch
from transformers import BertTokenizer, BertModel
from sklearn import preprocessing
import numpy as np
from dotenv import load_dotenv
sys.path.append("modules")
from check_enc import *


load_dotenv()
mongo_url=os.environ.get('MONGOURL')

# Load the BERT model and tokenizer


text = "This is an example sentence."
def get_sentence_embedding(text:str,tokenizer,model):
  # Preprocess the input text
  inputs = tokenizer.encode_plus(
      text,
      add_special_tokens=True,
      max_length=512,
      return_attention_mask=True,
      return_tensors='pt'
  )
  # Get the BERT embeddings
  outputs = model(**inputs)
  last_hidden_states = outputs.last_hidden_state

  # Extract the sentence embedding
  sentence_embedding = torch.mean(last_hidden_states, dim=1)
  return sentence_embedding[0]

def meta_function_embedding(tokenizer,model):
  return lambda text : get_sentence_embedding(text,tokenizer,model)

#print(get_sentence_embedding(text))

# !pip install torchmetrics
from torchmetrics.functional import pairwise_cosine_similarity

def concat_vector_embeddings(L1:list[str],L2:list[str]):
  x=torch.cat(tuple([get_sentence_embedding(e) for e in L1]),dim=0)
  y=torch.cat(tuple([get_sentence_embedding(e) for e in L2]),dim=0)
  return x,y

def most_similar(L1:list[str],L2:list[str]):
  x,y=concat_vector_embeddings(L1,L2)
  a=pairwise_cosine_similarity(x, y)
  print(a)
  return torch.argmax(a, dim=1)

# L1=["J'aime les burgers mcdonalds","voiture"]
# L2=["J'aime les burgers mcdonalds","J'aime les voitures","J'aime pas les voitures"]

# x,y=concat_vector_embeddings(L1,L2)

# print(most_similar(L1,L2))

# output = pairwise_cosine_similarity(x, y)
# print(output)


#model = get_model(2)
# !pip install pymongo
import pymongo
import pandas as pd


file_path="sentiment_dataset/train.csv"
le = preprocessing.LabelEncoder()

df=pd.read_csv(file_path, encoding=check_enc(file_path))
df.dropna(inplace=True)
df['sentiment'] = le.fit_transform(df['sentiment'])
# print(df[["text","sentiment"]].head())

# L=[]
# for index, row in df.iterrows():
#   embed=get_sentence_embedding_list(row['text'])
#   L.append(embed)
#   print(index)
# embeddings=pd.Series(L)
# df["embedding"]=embeddings
# print(df.columns)
# df.to_csv("sentiment_dataset/train_embed.csv", sep='\t')


# file_path="sentiment_dataset/train_embed.csv"
# df=pd.read_csv(file_path, encoding=check_enc(file_path), sep="\t")
# le = preprocessing.LabelEncoder()
# df['sentiment'] = le.fit_transform(df['sentiment'])

# print(eval(df[df["id"]==0]["embedding"][0])[0])
# Connect to your MongoDB database
# client = pymongo.MongoClient(mongo_url)
# db = client["RemiGOAT"]
# collection = db["Message"]

# k=0
# for x in collection.find():
#   criteria = {"_id": x["_id"]}
#   embed=get_sentence_embedding(x["body"]).detach().cpu().numpy().tolist()
#   update_data = {"$set": {"embedding": embed}}
#   result = collection.update_one(criteria, update_data)
#   k+=1
#   print(k)


import torch
from torch import nn

# Define the model class
class TorchTextClassifier(nn.Module):
  def __init__(self, input_dim, hidden_dim, hidden_dim_2, num_classes,tokenizer,model_transformer):
    super(TorchTextClassifier, self).__init__()
    # Define layers
    self.get_embedding = meta_function_embedding(tokenizer,model_transformer)
    self.linear1 = nn.Linear(input_dim, hidden_dim)
    self.relu = nn.ReLU()
    self.linear2 = nn.Linear(hidden_dim, hidden_dim_2)
    self.linear3 = nn.Linear(hidden_dim_2, num_classes)
    self.softmax = nn.Softmax(dim=-1)

  def forward(self, x:str):
    # Pass through layers
    x = self.get_embedding(x)
    x = self.linear1(x)
    x = self.relu(x)
    x = self.linear2(x)
    x = self.relu(x)
    x = self.linear3(x)
    x = self.softmax(x)
    return x

# Example usage
# Assuming your data is in a NumPy array called 'data' with shape (n_samples, input_dim)
# and labels in another NumPy array called 'labels' with shape (n_samples,)

# Define model parameters
input_dim = 768 # Get the number of features from data
hidden_dim = 64  # Define the size of the hidden layer
hidden_dim_2 = 32
num_classes = 3  # Get the number of unique categories in labels

tokenizer = BertTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')
model_transformer = BertModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')

# Create the model instance
model = TorchTextClassifier(input_dim, hidden_dim, hidden_dim_2, num_classes,tokenizer,model_transformer)

# Define loss function and optimizer (replace these based on your needs)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())

# Training loop
for epoch in range(10):  # loop over the dataset multiple times
    for index, row in df.iterrows():
        # get the inputs; data is a list of tuples (images, labels)
        inputs = row["text"]
        if row["sentiment"] == 0:
          labels=[1,0,0]
        elif row["sentiment"] == 1:
          labels=[0,1,0]
        else :
          labels=[0,0,1]
        labels=torch.tensor(labels)
        # zero the parameter gradients
        optimizer.zero_grad()

        # forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels.float())

        # backward pass
        loss.backward()

        # update the model parameters
        optimizer.step()

        if index % 100 == 99:  # print every 100 mini-batches
            print(f'Epoch {epoch+1}, Batch {index+1}, Loss: {loss.item()}')
# Prediction example
# Convert data to tensor
# test_data = torch.from_numpy(data[0]).float()  # Assuming you want to predict for the first sample

# Get the model prediction
# prediction = model(test_data)

# Get the predicted class index with the highest probability
# predicted_class = torch.argmax(prediction).item()

# print(f"Predicted class for the first sample: {predicted_class}")


